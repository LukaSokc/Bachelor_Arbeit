{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0. Bibliotheken =====\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from huggingface_hub import login            # Token sollte über ENV gesetzt sein\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "\n",
    "# NLP-Augmentation\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.model.word_dict.wordnet as nmw_wordnet\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "for pkg in (\"wordnet\", \"omw-1.4\", \"averaged_perceptron_tagger_eng\", \"punkt\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"corpora/{pkg}\")\n",
    "    except LookupError:\n",
    "        nltk.download(pkg, quiet=True)\n",
    "\n",
    "# nlpaug benötigt ein globales WordNet-Objekt\n",
    "nmw_wordnet.wordnet = wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2. Modell & Prozessor =====\n",
    "model_id = \"../models/Gemma_3_4B/merged_model_batchsize_2\"\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "DTYPE = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float32\n",
    ")\n",
    "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"🖥️ Torch device: {device_name} | Dtype: {DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3. Daten laden =====\n",
    "project_root = Path.cwd().parent\n",
    "data_path = project_root / \"data\" / \"validation\"\n",
    "dataset = load_from_disk(str(data_path))\n",
    "print(f\"Datensatz geladen: {len(dataset)} Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4. Text-Augmenter definieren =====\n",
    "text_augmenters = {\n",
    "    \"char_swap\": nac.RandomCharAug(action=\"swap\", aug_char_p=0.015, aug_char_min=1, aug_char_max=1),\n",
    "    \"typo_insert\": nac.RandomCharAug(action=\"insert\", aug_char_p=0.007, aug_char_min=1, aug_char_max=1),\n",
    "    \"sentence_shuffle\": naw.RandomWordAug(action=\"swap\", aug_p=0.15, aug_min=1, aug_max=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 5. CSV-Ausgabe vorbereiten =====\n",
    "output_csv = project_root / \"data\" / \"llm_answers\" / \"batchsize_2_textaugmented_results.csv\"\n",
    "output_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fieldnames = [\n",
    "    \"ID\",\n",
    "    \"augmentation\",\n",
    "    \"question_orig\",\n",
    "    \"question_augmented\",\n",
    "    \"correct_answer\",\n",
    "    \"model_output\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with output_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "    writer = csv.DictWriter(f_out, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    # ===== 6. Generierung =====\n",
    "    for aug_name, text_aug in text_augmenters.items():\n",
    "        print(f\"\\n==> Running text augmentation: {aug_name}\")\n",
    "        for idx in tqdm(range(len(dataset)), desc=aug_name):\n",
    "            sample = dataset[idx]\n",
    "\n",
    "            # Bild (PIL RGB)\n",
    "            image = sample[\"image\"].convert(\"RGB\")\n",
    "\n",
    "            # Frage augmentieren\n",
    "            question_orig = sample[\"question\"]\n",
    "            question_aug = text_aug.augment(question_orig)\n",
    "\n",
    "            ground_truth = sample[\"answer\"]\n",
    "            qid = sample.get(\"id\", idx + 1)\n",
    "\n",
    "            # Gemma-Chat-Prompt\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": (\n",
    "                                \"You are a medical pathology expert. Answer strictly \"\n",
    "                                \"based on the visual information in the image. Use short \"\n",
    "                                \"precise terms without explanations.\"\n",
    "                            ),\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image},\n",
    "                        {\"type\": \"text\", \"text\": question_aug},\n",
    "                    ],\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            try:\n",
    "                # Tokenisierung\n",
    "                inputs = processor.apply_chat_template(\n",
    "                    messages,\n",
    "                    add_generation_prompt=True,\n",
    "                    tokenize=True,\n",
    "                    return_dict=True,\n",
    "                    return_tensors=\"pt\",\n",
    "                ).to(model.device, dtype=DTYPE)\n",
    "\n",
    "                prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "                # Inferenz\n",
    "                with torch.inference_mode():\n",
    "                    gen_output = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=100,\n",
    "                        do_sample=False,\n",
    "                    )\n",
    "\n",
    "                answer = processor.decode(gen_output[0][prompt_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "                # CSV-Eintrag\n",
    "                writer.writerow(\n",
    "                    {\n",
    "                        \"ID\": qid,\n",
    "                        \"augmentation\": aug_name,\n",
    "                        \"question_orig\": question_orig,\n",
    "                        \"question_augmented\": question_aug,\n",
    "                        \"correct_answer\": ground_truth,\n",
    "                        \"model_output\": answer,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception as err:\n",
    "                print(f\"Fehler bei Sample {qid}, Augmentation {aug_name}: {err}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"\\n Alle Ergebnisse gesichert: {output_csv.relative_to(project_root)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

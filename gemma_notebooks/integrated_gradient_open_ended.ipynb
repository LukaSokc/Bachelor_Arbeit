{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageFilter\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from captum.attr import IntegratedGradients, visualization\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "import re\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Modell‑ und Processor‑Setup\n",
    "# -------------------------------------------------------------\n",
    "model_id = \"../models/Gemma_3_4B/merged_model_batchsize_2\"\n",
    "torch.manual_seed(8)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEBUG: device\", device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer = processor.tokenizer\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, trust_remote_code=True\n",
    ").to(device)\n",
    "model.eval().zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Hilfsfunktionen\n",
    "# -------------------------------------------------------------\n",
    "def load_image_from_url(image_path):\n",
    "    return Image.open(image_path).convert(\"RGB\")\n",
    "# Eigenes Colormap fuer Heatmaps\n",
    "default_cmap = LinearSegmentedColormap.from_list(\n",
    "    \"custom_blue\",\n",
    "    [(0, \"#ffffff\"), (0.25, \"#252b36\"), (1, \"#000000\")],\n",
    "    N=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Hauptfunktion: Sequenz‑Attribution\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "def vqa_vilt_interpret_seq_attr(\n",
    "    image_filename,\n",
    "    question,\n",
    "    true_answer,\n",
    "    model,\n",
    "    processor,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    max_gen_len=20\n",
    "):\n",
    "    \"\"\"Berechnet Bild‑ und Token‑Attribution für eine einzelne Frage.\"\"\"\n",
    "    # 1) Bild laden + verschwommene Baseline\n",
    "    img = load_image_from_url(image_filename)\n",
    "    blurred = img.filter(ImageFilter.GaussianBlur(radius=5))\n",
    "    blur_inputs = processor.apply_chat_template(\n",
    "        [{\"role\":\"user\",\"content\":[{\"type\":\"image\",\"image\":blurred}]}],\n",
    "        tokenize=True, return_dict=True, return_tensors=\"pt\",\n",
    "        padding=True, truncation=True,\n",
    "        do_pan_and_scan=False, add_generation_prompt=True\n",
    "    ).to(device)\n",
    "    blur_px      = blur_inputs[\"pixel_values\"] # Shape: [1, 3, H, W]\n",
    "    img_token_id = processor.tokenizer.image_token_id\n",
    "\n",
    "    # 2) Prompt (Bild + Frage) tokenisieren\n",
    "    inputs      = processor.apply_chat_template(\n",
    "        [{\"role\":\"user\",\"content\":[\n",
    "            {\"type\":\"image\",\"image\":img},\n",
    "            {\"type\":\"text\",\"text\":question}\n",
    "        ]}],\n",
    "        tokenize=True, return_dict=True, return_tensors=\"pt\",\n",
    "        padding=True, truncation=True,\n",
    "        do_pan_and_scan=False, add_generation_prompt=True\n",
    "    ).to(device)\n",
    "    px          = inputs[\"pixel_values\"]\n",
    "    input_ids   = inputs[\"input_ids\"]\n",
    "    attention   = inputs[\"attention_mask\"]\n",
    "    text_embeds = model.get_input_embeddings()(input_ids)\n",
    "    prompt_len  = input_ids.shape[1]\n",
    "\n",
    "\n",
    "    # 3) Antwort generieren und decodieren\n",
    "    gen_out   = model.generate(\n",
    "        pixel_values=px,\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention,\n",
    "        max_length=prompt_len + max_gen_len,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    full_ids  = gen_out.sequences.to(device)  # [1, total_len]\n",
    "    gen_ids   = full_ids[0, prompt_len:].tolist()\n",
    "    pred_ans  = processor.tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    # 4) Embeddings & Baselines vorbereiten\n",
    "    total_len    = full_ids.shape[1]\n",
    "    full_embeds  = model.get_input_embeddings()(full_ids)\n",
    "    pad_id       = processor.tokenizer.pad_token_id\n",
    "    base_ids     = torch.full_like(full_ids, pad_id)\n",
    "    base_embeds  = model.get_input_embeddings()(base_ids)\n",
    "    attn_full    = torch.ones_like(full_ids)\n",
    "\n",
    "    # 5) Custom forward: fügt Bild‑Features in Text‑Embeddings ein\n",
    "    def forward_joint(px_batch, txt_emb, attn_batch):\n",
    "        n_steps, seq_len, hid = txt_emb.shape\n",
    "        img_feats = model.get_image_features(px_batch)\n",
    "\n",
    "        # Bildplätze (<image_soft_token>) ersetzen\n",
    "        flat_txt  = txt_emb.view(-1, hid)\n",
    "        flat_img  = img_feats.view(-1, hid)\n",
    "        mask0     = (full_ids.view(-1) == img_token_id)\n",
    "        mask_all  = mask0.repeat(n_steps)\n",
    "        flat_txt[mask_all] = flat_img\n",
    "\n",
    "        merged    = flat_txt.view(n_steps, seq_len, hid)\n",
    "        expanded_attn = attn_batch.repeat(n_steps, 1)\n",
    "        logits    = model(\n",
    "            input_ids=None,\n",
    "            pixel_values=None,\n",
    "            inputs_embeds=merged,\n",
    "            attention_mask=expanded_attn,\n",
    "            use_cache=False,\n",
    "        ).logits\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        # Summe der Log-Prob fuer jeden Step\n",
    "        out = []\n",
    "        for i in range(n_steps):\n",
    "            lp = torch.zeros((), device=log_probs.device)\n",
    "            for pos in range(prompt_len, seq_len):\n",
    "                tok = full_ids[0, pos]\n",
    "                lp = lp + log_probs[i, pos, tok]\n",
    "            out.append(lp)\n",
    "        return torch.stack(out)\n",
    "\n",
    "    # 6) Integrated Gradients\n",
    "    px.requires_grad_(True)\n",
    "    full_embeds.requires_grad_(True)\n",
    "    ig = IntegratedGradients(forward_joint)\n",
    "    attr_img, attr_txt = ig.attribute(\n",
    "        inputs=(px, full_embeds),\n",
    "        baselines=(blur_px, base_embeds),\n",
    "        additional_forward_args=(attn_full,),\n",
    "        method = \"riemann_trapezoid\",\n",
    "        n_steps=300,\n",
    "        internal_batch_size=1\n",
    "    )\n",
    "\n",
    "     # 7) Bild‑Attribution \n",
    "    orig_np = np.array(img)\n",
    "    img_attr = attr_img.squeeze(0).cpu().detach().numpy().transpose(1,2,0)\n",
    "\n",
    "    attr_np = attr_img[0].squeeze(0).detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    h_orig, w_orig = img.size[1], img.size[0]      # PIL: .size = (W,H)\n",
    "    attr_up = cv2.resize(attr_np, (w_orig, h_orig), interpolation=cv2.INTER_LINEAR)\n",
    "    # 8) Text‑Attribution filtern & normieren\n",
    "    raw_text_attr = attr_txt.sum(dim=2).squeeze(0).cpu().detach().numpy()\n",
    "    tokenized     = processor.tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "    special       = set(processor.tokenizer.all_special_tokens)\n",
    "    extra         = {\"user\",\"model\",\"#start_of_turn\",\"#end_of_turn\"}\n",
    "    special_tokens = set(processor.tokenizer.all_special_tokens)\n",
    "    filtered = []\n",
    "    for tok, attr in zip(tokenized, raw_text_attr):\n",
    "        clean = tok.lstrip(\"▁\")\n",
    "        if re.match(r\"^<.*>$\", tok):\n",
    "            continue\n",
    "        if tok in special_tokens:\n",
    "            continue\n",
    "        if clean.lower() in {\"user\", \"model\"}:\n",
    "            continue\n",
    "\n",
    "        filtered.append((clean, attr))\n",
    "\n",
    "    if filtered:\n",
    "        tokens_real, text_attr_real = zip(*filtered)\n",
    "        text_attr_real = np.array(text_attr_real)\n",
    "        text_attr_real /= np.linalg.norm(text_attr_real) + 1e-8\n",
    "    else:\n",
    "        tokens_real, text_attr_real = [], np.array([])\n",
    "    # 9) Visualisierung\n",
    "    vis_data = [ visualization.VisualizationDataRecord(\n",
    "        text_attr_real,                  # attribution scores pro Token\n",
    "        1.0,                             # pred_prob (dummy)\n",
    "        pred_ans,                        # pred_class\n",
    "        true_answer,                     # true_class\n",
    "        true_answer,                     # attr_class\n",
    "        text_attr_real.sum(),            # attr_score\n",
    "        tokens_real,                     # raw_input_ids\n",
    "        0.0                              # convergence_score\n",
    "    )]\n",
    "    visualization.visualize_text(vis_data)\n",
    "    orig_img = np.asarray(img)\n",
    "    orig_img = np.asarray(img)   # PIL → ndarray\n",
    "    visualization.visualize_image_attr_multiple(\n",
    "        attr_up, \n",
    "        orig_img,\n",
    "        [\"original_image\", \"heat_map\"], [\"all\", \"absolute_value\"],\n",
    "        titles=[\"Original Image\", \"Attribution Magnitude\"],\n",
    "        cmap=default_cmap,\n",
    "        show_colorbar=True,\n",
    "    )\n",
    "    # 10) Konsolenausgabe\n",
    "    print(\"Image-Beitrag:\", attr_img.sum().item())\n",
    "    print(\"Text-Beitrag: \", text_attr_real.sum())\n",
    "    print(\"Total-Beitrag:\", attr_img.sum().item() + text_attr_real.sum())\n",
    "    print(\"-\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Skripteintrittspunkt (Beispiel‑Aufruf)\n",
    "# -------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    image_path   = \"uterus.png\"\n",
    "    questions    = [\"where does this part belong to?\"]\n",
    "    true_answers = [\"female reproductive system\"]\n",
    "\n",
    "    for q, a in zip(questions, true_answers):\n",
    "        vqa_vilt_interpret_seq_attr(\n",
    "            image_filename=image_path,  # hier dein Bildpfad\n",
    "            question=q,\n",
    "            true_answer=a,\n",
    "            model=model,\n",
    "            processor=processor,\n",
    "            device=device,\n",
    "            max_gen_len=20\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

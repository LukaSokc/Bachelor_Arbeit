{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Gemma 3 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch>=2.4.0\" tensorboard torchvision\n",
    "\n",
    "# Install Gemma release branch from Hugging Face\n",
    "%pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"datasets==3.3.2\" \\\n",
    "  \"accelerate==1.4.0\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.45.3\" \\\n",
    "  \"trl==0.15.2\" \\\n",
    "  \"peft==0.14.0\" \\\n",
    "  \"pillow==11.1.0\" \\\n",
    "  protobuf \\\n",
    "  pip install bitsandbytes trl peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig, TrainerCallback, TrainerControl, TrainerState\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model\n",
    "from transformers import AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 1. Authentifizierung und Datensatz laden\n",
    "# ---------------------------------------------------------------------------\n",
    "login(\"hf_login\")  # Token hier sicher speichern\n",
    "ds = load_dataset(\"flaviagiammarino/path-vqa\")\n",
    "dataset = ds['train']\n",
    "eval_dataset = ds['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 2. Validierungs‑Subset definieren\n",
    "# ---------------------------------------------------------------------------\n",
    "indices_file = Path(\"validation_subset_indices.txt\")\n",
    "with indices_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    selected_indices = [int(line.strip()) for line in f if line.strip()]\n",
    " \n",
    "# Filtere den Validierungsdatensatz\n",
    "filtered_val_dataset = eval_dataset.select(selected_indices)\n",
    " \n",
    "print(\"Anzahl der ausgewählten Einträge:\", len(filtered_val_dataset))\n",
    "eval_dataset = filtered_val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 3. Hilfsfunktionen\n",
    "# ---------------------------------------------------------------------------\n",
    "# System message\n",
    "system_message = \"You are a medical pathology expert specializing in visual diagnosis. Your task is to answer questions based only on the provided histopathology image and the question. Do not use any external knowledge or assumptions. Your answers must be medically accurate, concise, and grounded in visible features of the image.\"\n",
    "\n",
    "# Konvertiert ein Datensatz‑Sample in das OAI Chat‑Format.\n",
    "def format_data(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\", \"image\": sample[\"image\"]},\n",
    "        {\"type\": \"text\", \"text\": f\"Question: {sample['question']}\\nAnswer based only on the image.\"}\n",
    "    ]},\n",
    "    {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": sample[\"answer\"]}]}\n",
    "],\n",
    "    }\n",
    "\n",
    "\n",
    "def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n",
    "    image_inputs = []\n",
    "    # Iterate through each conversation\n",
    "    for msg in messages:\n",
    "        # Get content (ensure it's a list)\n",
    "        content = msg.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "\n",
    "        # Check each content element for images\n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and (\n",
    "                \"image\" in element or element.get(\"type\") == \"image\"\n",
    "            ):\n",
    "                # Get the image and convert to RGB\n",
    "                if \"image\" in element:\n",
    "                    image = element[\"image\"]\n",
    "                else:\n",
    "                    image = element\n",
    "                image_inputs.append(image.convert(\"RGB\"))\n",
    "    return image_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 4. Daten vorbereiten\n",
    "# ---------------------------------------------------------------------------\n",
    "dataset = [format_data(sample) for sample in dataset]\n",
    "eval_dataset = [format_data(sample) for sample in eval_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 5. Modell konfigurieren\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Hugging Face model id\n",
    "model_id = \"google/gemma-3-4b-it\" \n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.text_config.use_cache = False\n",
    "\n",
    "# Definition von init Parametern\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",  \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    text_config=config.text_config\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"], \n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "# Lade Modell und Prozessor\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "processor.tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA‑Adapter konfigurieren\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=['q_proj','v_proj'],\n",
    "    task_type=\"CAUSAL_LM\")   \n",
    "\n",
    "print(f\"Befor adapter parameters: {model.num_parameters()}\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 6. Training konfigurieren\n",
    "# ---------------------------------------------------------------------------\n",
    "args = SFTConfig(\n",
    "    output_dir=\"gemma-product-description\",     \n",
    "    num_train_epochs=2,                         \n",
    "    per_device_train_batch_size=2,              \n",
    "    per_device_eval_batch_size = 2,             \n",
    "    gradient_accumulation_steps=1,              \n",
    "    eval_steps = 2000,\n",
    "    eval_strategy = 'steps',\n",
    "    gradient_checkpointing=True,                \n",
    "    optim=\"paged_adamw_32bit\",                  \n",
    "    logging_steps=1000,                        \n",
    "    save_steps = 2000,\n",
    "    save_strategy=\"steps\",                      \n",
    "    learning_rate=2e-4,                         \n",
    "    bf16=True,                                  \n",
    "    metric_for_best_model = 'eval_loss',\n",
    "    load_best_model_at_end = True,\n",
    "    max_grad_norm=0.3,                         \n",
    "    warmup_ratio=0.1,                           \n",
    "    max_seq_length=128,                        \n",
    "    lr_scheduler_type=\"constant\",               \n",
    "    push_to_hub=True,                           \n",
    "    report_to=\"tensorboard\",                    \n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  \n",
    "    dataset_text_field=\"\",                      \n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  \n",
    ")\n",
    "args.remove_unused_columns = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellt einen Batch aus Text‑ und Bild‑Inputs für das Modell\n",
    "def collate_fn(examples):\n",
    "    texts = []\n",
    "    images = []\n",
    "    for example in examples:\n",
    "        image_inputs = process_vision_info(example[\"messages\"])\n",
    "        text = processor.apply_chat_template(\n",
    "            example[\"messages\"], add_generation_prompt=False, tokenize=False\n",
    "        )\n",
    "        texts.append(text.strip())\n",
    "        images.append(image_inputs)\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "\n",
    "    # Mask image tokens\n",
    "    image_token_id = [\n",
    "        processor.tokenizer.convert_tokens_to_ids(\n",
    "            processor.tokenizer.special_tokens_map[\"boi_token\"]\n",
    "        )\n",
    "    ]\n",
    "    # Mask tokens for not being used in the loss computation\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    labels[labels == image_token_id] = -100\n",
    "    labels[labels == 262144] = -100\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config = peft_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 7. Training und Evaluation\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"-\"*30)\n",
    "print(\"Evaluating\")\n",
    "metric = trainer.evaluate()\n",
    "print(metric)\n",
    "print(\"-\"*30)\n",
    "print(\"Training\")\n",
    "trainer.train()\n",
    "print(\"-\"*30)\n",
    "print(\"Saving Model to Hugging Face Hub\")\n",
    "trainer.save_model()\n",
    "print(\"-\"*30)\n",
    "print(\"Congratulations you have succsesfully finetuned Gemma 3 4b it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# 8. LoRA‑Gewichte mergen\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Basemodel Laden\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, low_cpu_mem_usage=True)\n",
    "\n",
    "# Merge von Basemodel und LoRA Gewichten\n",
    "peft_model = PeftModel.from_pretrained(model, args.output_dir)\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"merged_model_batchsize_2\", safe_serialization=True, max_shard_size=\"2GB\")\n",
    "processor = AutoProcessor.from_pretrained(args.output_dir)\n",
    "processor.save_pretrained(\"merged_model_batchsize_2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

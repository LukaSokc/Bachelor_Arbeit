{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import  AutoProcessor\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datasets import load_from_disk\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig, TrainerCallback, TrainerControl, TrainerState\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig\n",
    "from trl import SFTTrainer\n",
    "from peft import get_peft_model\n",
    "from transformers import AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\"hf_login\")\n",
    "# Datensätze laden (Trainings- und Validierungssplit)\n",
    "dataset = load_dataset(\"flaviagiammarino/path-vqa\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "\n",
    "print(\"Datasets wurden geladen.\")\n",
    "print(\"Trainingsgröße:\", len(train_dataset))\n",
    "print(\"Validierungsgröße:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 2: Few-Shot-Beispiele vorbereiten\n",
    "few_shot_indices = [10658, 18497, 8273, 16324, 10392, 9073, 4623, 10336]\n",
    "few_shot_examples = []\n",
    "\n",
    "for idx in few_shot_indices:\n",
    "    sample = train_dataset[idx]\n",
    "    few_shot_examples.append({\n",
    "        \"question\": sample[\"question\"],\n",
    "        \"answer\": sample[\"answer\"],\n",
    "        \"image\": sample[\"image\"]\n",
    "    })\n",
    "\n",
    "print(\"Few-Shot-Beispiele vorbereitet. Anzahl:\", len(few_shot_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle 3: Nachrichtenliste (Prompt) erstellen\n",
    "\n",
    "# Wähle ein Beispiel aus dem Validierungsdatensatz (z.B. das erste Sample)\n",
    "sample = val_dataset[0]\n",
    "val_image = sample[\"image\"]\n",
    "val_question = sample[\"question\"]\n",
    "\n",
    "# System Message definieren\n",
    "system_message = (\n",
    "    \"You are a medical pathology expert. Your task is to answer medical questions \"\n",
    "    \"based solely on the visual information in the provided pathology image. \"\n",
    "    \"Focus only on what is visible in the image — do not rely on prior medical knowledge, \"\n",
    "    \"assumptions, or external information. Your responses should be short, factual, \"\n",
    "    \"and medically precise, using appropriate terminology. \"\n",
    "    \"Do not include any explanations, reasoning, or additional text. \"\n",
    "    \"Use a consistent format, without punctuation, and avoid capitalisation unless medically required. \"\n",
    "    \"Only return the exact answer.\"\n",
    ")\n",
    "\n",
    "# Erstelle die Nachrichtenliste\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Füge alle Few-Shot-Beispiele hinzu, wobei Bild, Frage und Antwort in einer Nachricht kombiniert werden.\n",
    "for ex in few_shot_examples:\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": ex[\"image\"]},\n",
    "            {\"type\": \"text\", \"text\": \"question: \" + ex[\"question\"] + \"\\nanswer: \" + ex[\"answer\"]}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Füge das Validierungssample hinzu (nur Frage, da hier das Modell antworten soll)\n",
    "messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": val_image},\n",
    "            {\"type\": \"text\", \"text\": \"question: \" + val_question},\n",
    "            {\"type\": \"text\", \"text\": \"Answer: \"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "# Zeige die Nachrichtenliste an\n",
    "print(\"Nachrichtenliste für den Prompt:\")\n",
    "for i, msg in enumerate(messages):\n",
    "    print(f\"Nachricht {i+1}: {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_vision_info(messages: list[dict]) -> tuple[list[Image.Image], list]:\n",
    "    image_inputs = []\n",
    "    # Durchlaufe alle Nachrichten\n",
    "    for msg in messages:\n",
    "        # Hole den Inhalt (sicherstellen, dass es eine Liste ist)\n",
    "        content = msg.get(\"content\", [])\n",
    "        if not isinstance(content, list):\n",
    "            content = [content]\n",
    "\n",
    "        # Ueberpruefe jeden Inhalt auf Bilder\n",
    "        for element in content:\n",
    "            if isinstance(element, dict) and (\"image\" in element or element.get(\"type\") == \"image\"):\n",
    "                # Hole das Bild und konvertiere es in RGB\n",
    "                image = element.get(\"image\", element)\n",
    "                image_inputs.append(image.convert(\"RGB\"))\n",
    "    return image_inputs, []  # Leere Liste für Videos, falls keine vorhanden sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalen Prompt und Modell-Input erzeugen und Antworten für 10 Beispiele generieren\n",
    "model_id = \"google/gemma-3-4b-it\" \n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "\n",
    "# Cache für Gemma ausschalten \n",
    "config.text_config.use_cache = False\n",
    "\n",
    "# Definition von model init Argumenten\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",    \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    text_config=config.text_config\n",
    ")\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
    "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
    ")\n",
    "\n",
    "# Lade Modell und Prozessor\n",
    "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "processor.tokenizer.padding_side = 'right'\n",
    "\n",
    "# Iteriere über 10 Beispiele aus dem Validierungsdatensatz\n",
    "for idx in range(10):\n",
    "    sample = val_dataset[idx]\n",
    "    val_image = sample[\"image\"]\n",
    "    val_question = sample[\"question\"]\n",
    "\n",
    "    # System Message\n",
    "    system_message = (\n",
    "        \"You are a medical pathology expert. Your task is to answer medical questions \"\n",
    "        \"based solely on the visual information in the provided pathology image. \"\n",
    "        \"Focus only on what is visible in the image — do not rely on prior medical knowledge, \"\n",
    "        \"assumptions, or external information. Your responses should be short, factual, \"\n",
    "        \"and medically precise, using appropriate terminology. \"\n",
    "        \"Do not include any explanations, reasoning, or additional text. \"\n",
    "        \"Use a consistent format, without punctuation, and avoid capitalisation unless medically required. \"\n",
    "        \"Only return the exact answer.\"\n",
    "    )\n",
    "\n",
    "    # Erstelle die initiale Nachrichtenliste mit der System Message\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Füge alle Few-Shot-Beispiele hinzu: \n",
    "    for ex in few_shot_examples:\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": ex[\"image\"]},\n",
    "                {\"type\": \"text\", \"text\": \"question: \" + ex[\"question\"]}\n",
    "            ]\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"answer: \" + ex[\"answer\"]}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    # Füge das Validierungsbeispiel hinzu \n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": val_image},\n",
    "            {\"type\": \"text\", \"text\": \"question: \" + val_question},\n",
    "            {\"type\": \"text\", \"text\": \"Answer: \"}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Anzeigen der Nachrichtenliste für das aktuelle Beispiel\n",
    "    print(\"\\n=== Nachrichtenliste für Beispiel\", idx + 1, \"===\")\n",
    "    for i, msg in enumerate(messages):\n",
    "        print(f\"Nachricht {i+1}: {msg}\")\n",
    "\n",
    "    # Erzeuge den Text-Prompt aus der gesamten Nachrichtenliste\n",
    "    text_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    print(\"\\nErzeugter Text-Prompt für Beispiel\", idx + 1, \":\\n\", text_prompt)\n",
    "\n",
    "    # Verarbeite die Vision-Informationen (Bilder und Videos)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "    # Erstelle die finalen Inputs für das Modell\n",
    "    inputs = processor(\n",
    "        text=[text_prompt],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "\n",
    "    # Generiere die Antwort ohne Gradientenberechnung\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "        # Entferne den Input-Teil, um nur die generierte Antwort zu erhalten\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "    generated_answer = output_text[0]\n",
    "    print(\"Generierte Antwort für Beispiel\", idx + 1, \":\", generated_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

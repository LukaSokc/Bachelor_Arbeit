{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 0. Bibliotheken =====\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8f29e0118446738a466ec0d72dbead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ===== 1. Modell & Prozessor =====\n",
    "MODEL_ID = \"merged_model_batchsize_2/\"\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map={\"\": 0},\n",
    "    torch_dtype=torch.bfloat16,\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, use_fast=True)\n",
    "\n",
    "DTYPE = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float32\n",
    ")\n",
    "device_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
    "print(f\"🖥️ Torch device: {device_name} | Dtype: {DTYPE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🖥️ Torch device: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "# ===== 2. Daten laden =====\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"validation\"\n",
    "dataset = load_from_disk(str(DATA_PATH))\n",
    "print(f\"Dataset geladen: {len(dataset)} Samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17650/30055526.py:4: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
      "/tmp/ipykernel_17650/30055526.py:6: UserWarning: Argument(s) 'max_holes, max_h_size, max_w_size' are not valid for transform CoarseDropout\n",
      "  A.CoarseDropout(max_holes=5, max_h_size=32, max_w_size=32, p=0.3),\n",
      "/tmp/ipykernel_17650/30055526.py:7: UserWarning: Argument(s) 'quality_lower, quality_upper' are not valid for transform ImageCompression\n",
      "  A.ImageCompression(quality_lower=70, quality_upper=100, p=0.3),\n"
     ]
    }
   ],
   "source": [
    "# ===== 3. Bild-Augmentierung =====\n",
    "augment = A.Compose(\n",
    "    [\n",
    "        A.Rotate(limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(0.1, 0.1, p=0.5),\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
    "        A.MotionBlur(blur_limit=3, p=0.3),\n",
    "        A.CoarseDropout(max_holes=5, max_h_size=32, max_w_size=32, p=0.3),\n",
    "        A.ImageCompression(70, 100, p=0.3),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4. CSV-Ausgabe vorbereiten =====\n",
    "OUTPUT_CSV = PROJECT_ROOT / \"data\" / \"llm_answers\" / \"batchsize_2_imageaug_results.csv\"\n",
    "OUTPUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FIELDNAMES = [\"ID\", \"question\", \"correct_answer\", \"model_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6259 [00:00<?, ?it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `64` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 429/6259 [02:20<34:51,  2.79it/s]/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/PIL/TiffImagePlugin.py:950: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 6259/6259 [34:41<00:00,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All results saved to bildaugmentattion.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with OUTPUT_CSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as f_out:\n",
    "    writer = csv.DictWriter(f_out, fieldnames=FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "\n",
    "    # ===== 5. Generierung =====\n",
    "    for idx in tqdm(range(len(dataset)), desc=\"Samples\"):\n",
    "        sample = dataset[idx]\n",
    "\n",
    "        # Bild laden und augmentieren\n",
    "        image_rgb = sample[\"image\"].convert(\"RGB\")\n",
    "        aug_np = augment(image=np.array(image_rgb))[\"image\"]\n",
    "        aug_image = Image.fromarray(aug_np)\n",
    "\n",
    "        question = sample[\"question\"]\n",
    "        ground_truth = sample[\"answer\"]\n",
    "        qid = sample.get(\"id\", idx + 1)\n",
    "\n",
    "        # Gemma-Chat-Prompt\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": (\n",
    "                            \"You are a medical pathology expert. \"\n",
    "                            \"Answer strictly based on the visual information in the image. \"\n",
    "                            \"Use short precise terms without explanations.\"\n",
    "                        ),\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": aug_image},\n",
    "                    {\"type\": \"text\", \"text\": question},\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            # Tokenisierung\n",
    "            inputs = processor.apply_chat_template(\n",
    "                messages,\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_dict=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(model.device, dtype=DTYPE)\n",
    "\n",
    "            prompt_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "            # Inferenz\n",
    "            with torch.inference_mode():\n",
    "                gen = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "\n",
    "            answer = processor.decode(gen[0][prompt_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "            # CSV-Eintrag\n",
    "            writer.writerow(\n",
    "                {\n",
    "                    \"ID\": qid,\n",
    "                    \"question\": question,\n",
    "                    \"correct_answer\": ground_truth,\n",
    "                    \"model_output\": answer,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as err:\n",
    "            print(f\"Fehler bei Sample {qid}: {err}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nAlle Ergebnisse gesichert: {OUTPUT_CSV.relative_to(PROJECT_ROOT)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
